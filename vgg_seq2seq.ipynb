{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers               import Input, Dense, GRU, LSTM, RepeatVector\n",
    "from keras.models               import Model, Sequential\n",
    "from keras.layers.core          import Flatten\n",
    "from keras.callbacks            import LambdaCallback\n",
    "from keras.optimizers           import SGD, RMSprop, Adam\n",
    "from keras.layers.wrappers      import Bidirectional as Bi\n",
    "from keras.layers.wrappers      import TimeDistributed as TD\n",
    "from keras.layers               import merge\n",
    "from keras.applications.vgg16   import VGG16 \n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers.noise         import GaussianNoise as GN\n",
    "import numpy as np\n",
    "import sys, os, io, json, re\n",
    "import random\n",
    "from keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"学習データの準備\"\"\"\n",
    "\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "# m = MeCab.Tagger('-Ochasen')\n",
    "m = MeCab.Tagger(' -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "m.parse('')  #バグ回避\n",
    "\n",
    "data = pd.read_csv('boketeScrapy/legend.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "テキスト抽出\n",
    "\n",
    "　形態素解析してリスト形式にする\n",
    "\"\"\"\n",
    "\n",
    "import mojimoji\n",
    "\n",
    "w_ys = []\n",
    "for txt in data['txt']:\n",
    "    txt_list = txt.split(\",\")\n",
    "    for txt in txt_list:\n",
    "        # 形態素解析\n",
    "        ys = []\n",
    "        tmp_ys = []\n",
    "        only_symbol = True\n",
    "        node = m.parseToNode(txt)\n",
    "        while node:\n",
    "            # surfaceで単語、featureで解析結果が取得できる\n",
    "            surface = mojimoji.han_to_zen(node.surface, digit=False)\n",
    "            features = node.feature.split(\",\")\n",
    "            # feature = [品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音]\n",
    "            if features[0] == \"BOS/EOS\" and features[-3] == \"*\":\n",
    "                node = node.next\n",
    "                continue\n",
    "            if features[0] == \"記号\":\n",
    "                # 基本的に記号がキレイに処理できない様なので無視するが、形態素解析結果が全て記号だった時はそのまま使用する\n",
    "                tmp_ys.append(surface)\n",
    "            else:\n",
    "                ys.append(surface)\n",
    "                only_symbol = False\n",
    "            node = node.next\n",
    "\n",
    "        if only_symbol is True:\n",
    "            tmp_ys.append(\"EOS\")\n",
    "            ys = tmp_ys\n",
    "        else:\n",
    "            ys.append(\"EOS\")\n",
    "        w_ys.append(ys)\n",
    "\n",
    "# print(w_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "単語のIndex作成\n",
    "\n",
    "形態素解析した単語の一覧辞書を作る\n",
    "'''\n",
    "\n",
    "import collections\n",
    "\n",
    "corpus = []\n",
    "for word in w_ys:\n",
    "    corpus.extend(word)\n",
    "\n",
    "\"\"\"\n",
    "登場回数降順のタプルにする\n",
    "collections.Counter(corpus).items(): 単語がkey, 出現回数がvalueとなる辞書を作り、[(key, val), (key, val), ...]に変換\n",
    "key=lambda x:x[1]: sortedでvalをソートキーに指定\n",
    "reverse=True: 降順\n",
    "\"\"\"\n",
    "word_id = [ (k,v) for k,v in sorted(collections.Counter(corpus).items(), key=lambda x:x[1], reverse=True) if k!=\"EOS\" ]\n",
    "# {単語:（１から始まる）ID}という辞書を作成\n",
    "word_id = { k:e for e, (k,v) in enumerate(word_id, start = 1)}\n",
    "word_id[\"EOS\"] = 0\n",
    "# print(word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "逆引き辞書作成\n",
    "\n",
    "{ID: 単語}という辞書を作成\n",
    "'''\n",
    "\n",
    "id_word = {v:k for k,v in word_id.items()}\n",
    "# print(id_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All words:\\t\\t\", len(word_id))\n",
    "# 平均単語数\n",
    "print(\"Avg. boke length:\\t\", np.mean(np.array([len(w) for w in w_ys])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "単語リストのパディング\n",
    "\n",
    "MAX_LEN = 最大単語数\n",
    "（上記の平均単語数を元に決める）\n",
    "''' \n",
    "\n",
    "MAX_LEN = 20\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 形態素解析した単語を全てIDに変換する\n",
    "id_ys = [[word_id[w] for w in ws] for ws in w_ys]\n",
    "\n",
    "# MAX_LENを超えた要素に対して、超過分のtruncate、不足分を\"EOS\"でpddingを行う処理\n",
    "# 返り値は２次元numpy配列\n",
    "vec_ys = pad_sequences(id_ys, maxlen=MAX_LEN, padding='post', truncating=\"post\", value=word_id[\"EOS\"])\n",
    "\n",
    "print(\"vector shape:\\t\",vec_ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "単語リストをone-hot(0-1)化\n",
    "'''\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# print(vec_ys)\n",
    "# [277 114 147   6 236 251   9  46  86  13]\n",
    "\n",
    "# to_categorical(): クラスベクトルa（0からlen(word_id)までの整数）を categorical_crossentropyとともに用いるためのバイナリのクラス行列に変換\n",
    "Y = np.array([to_categorical(a, len(word_id)) for a in vec_ys])\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"画像読み込み\"\"\"\n",
    "X = []\n",
    "for image_txt in data['images']:\n",
    "    image_list = image_txt.strip('[]')\n",
    "    images = re.findall(r'{.+?}', image_list)\n",
    "    for image in images:\n",
    "        image = image.replace(\"'\", '\"')\n",
    "        image = json.loads(image)\n",
    "        path = 'boketeScrapy/img/' + image['path']\n",
    "        img = load_img(path, target_size=(150, 150))\n",
    "        x = img_to_array(img)\n",
    "        X.append(x)\n",
    "\n",
    "X = np.asarray(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "モデル生成\n",
    "\n",
    "Getting started with the Keras functional API: https://keras.io/ja/getting-started/functional-api-guide/\n",
    "\"\"\"\n",
    "\n",
    "units = 1024\n",
    "\n",
    "input_tensor = Input(shape=(150, 150, 3))\n",
    "vgg_model = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "vgg_x = vgg_model.layers[-1].output\n",
    "vgg_x = Flatten()(vgg_x)\n",
    "vgg_x = Dense(units)(vgg_x)\n",
    "\n",
    "timestep = MAX_LEN\n",
    "# 入力が文章ならRepeatVectorの数は単語数になるが、今回は画像なので適正値が不明\n",
    "inputs = RepeatVector(MAX_LEN)(vgg_x)\n",
    "encoded = LSTM(units)(inputs)\n",
    "encoder = Model(input_tensor, encoded)\n",
    "\n",
    "DIM = len(word_id)\n",
    "x = RepeatVector(MAX_LEN)(encoded)\n",
    "x = Bi(LSTM(units, return_sequences=True))(x)\n",
    "# 入力次元は単語辞書と同じサイズになる\n",
    "decoded = TD(Dense(DIM, activation='softmax'))(x)\n",
    "\n",
    "model = Model(input_tensor, decoded)\n",
    "\n",
    "# for i, layer in enumerate(model.layers): # default 15\n",
    "#   print( i, layer )\n",
    "\n",
    "for layer in model.layers[:18]: # 18 is max of VGG16\n",
    "  layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#  下記も検証してみる\n",
    "# optimizer=' rmsprop'\n",
    "# loss=binary_crossentropy\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"学習実行\"\"\"\n",
    "epoch_cnt = 1000\n",
    "batch_size = 32  # データ数が少ないので、kerasのdefaultをそのまま使用\n",
    "\n",
    "path = 'model/weights.{epoch:02d}-{loss:.2f}-{acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=6, verbose=1, mode='auto')\n",
    "# history = model.fit(X, Y,  shuffle=True, batch_size=batch_size, epochs=epoch_cnt, verbose=1, validation_split=0.1, callbacks=[model_checkpoint, early_stopping] )\n",
    "history = model.fit(X, Y,  shuffle=True, batch_size=batch_size, epochs=epoch_cnt, verbose=1, callbacks=[model_checkpoint, early_stopping] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習結果をグラフ表示\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot results\n",
    "loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Loss')\n",
    "epochs = len(loss)\n",
    "plt.plot(range(epochs), loss, marker='.', label='loss')\n",
    "# plt.plot(range(epochs), val_loss, marker='.', label='val_loss')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(range(epochs), acc, marker='.', label='acc')\n",
    "# plt.plot(range(epochs), val_acc, marker='.', label='val_acc')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"学習済みモデルをロード\"\"\"\n",
    "\n",
    "model.load_weights('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ボケ実行\n",
    "\n",
    "# 画像をnp.array()にする\n",
    "# 未学習画像\n",
    "# f = \"f5b031451d0180c14f14844a48b97f08_600\"\n",
    "# path_test = \"img/\" + f + \".jpg\"\n",
    "\n",
    "# スクレイピングしてきた画像\n",
    "f = \"0c07c5fa50ed68ac2a84db5b134cf6a06d4b2276\"\n",
    "path_test = \"boketeScrapy/img/full/\" + f  + \".jpg\"\n",
    "\n",
    "X_test = []\n",
    "img_test = load_img(path_test, target_size=(150, 150))\n",
    "x_test = img_to_array(img_test)\n",
    "X_test.append(x_test)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "# 予測実行\n",
    "pred = model.predict(X_test)\n",
    "print(pred.shape)\n",
    "print(pred[0])\n",
    "\n",
    "# 予測結果を元にid_wordから文字列を抽出\n",
    "for p in pred[0]:\n",
    "    if np.argmax(p) == 0:\n",
    "        continue\n",
    "    print(id_word[int(np.argmax(p))], end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bokete_ai",
   "language": "python",
   "name": "bokete_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
